import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Load datasets
df_stats = pd.read_csv('nfl_statistics.csv')
df_injuries = pd.read_csv('nfl_major_injuries.csv')
df_history = pd.read_csv('nfl_injury_data_2012_2017.csv')

# Merge datasets
data = df_stats.merge(df_injuries, on=['Player', 'Season'], how='left')
data = data.merge(df_history, on=['Player'], how='left')

# Fill missing injury data
data['Injury'] = data['Injury'].fillna('No Injury')

# Create binary target variable
data['High_Risk'] = data['Injury'].apply(lambda x: 1 if x != 'No Injury' else 0)

# Feature selection
features = ['Age', 'Games_Played', 'Snaps', 'Tackles', 'Rushing_Attempts', 'Passing_Attempts', 'Yards']
X = data[features].fillna(0)
y = data['High_Risk']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Models
def evaluate_model(model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_probs = model.predict_proba(X_test)[:,1]
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
    plt.title("Confusion Matrix")
    plt.show()
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_probs):.2f}')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.show()

# Logistic Regression
print("\nLogistic Regression:")
evaluate_model(LogisticRegression())

# Random Forest Classifier
print("\nRandom Forest Classifier:")
evaluate_model(RandomForestClassifier(n_estimators=100, random_state=42))

# XGBoost Classifier
print("\nXGBoost Classifier:")
evaluate_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'))

# Feature Importance (Random Forest)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
feat_importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)
plt.figure(figsize=(8,5))
sns.barplot(x=feat_importances.values, y=feat_importances.index)
plt.title("Feature Importance for Injury Risk")
plt.show()
